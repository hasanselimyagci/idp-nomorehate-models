{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers datasets"
      ],
      "metadata": {
        "id": "LcXVb7udJHna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "N0elHhI0JS9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the dataset:\n",
        "- Directly from csv file or,\n",
        "- Using Hugging Face's datasets library (https://huggingface.co/docs/datasets/en/index)"
      ],
      "metadata": {
        "id": "9aKeoNTfpP9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('hateEn.csv')\n",
        "# or\n",
        "\n",
        "# dataset = load_dataset(\"ruanchaves/hatebr\")\n",
        "# dataset = pd.DataFrame(dataset['train']) # if it has a 'train' split"
      ],
      "metadata": {
        "id": "AZaoa_OHqpXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possible initial information and modification"
      ],
      "metadata": {
        "id": "NvfvkG6eqRyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To return column names, size and data type info\n",
        "dataset.info()"
      ],
      "metadata": {
        "id": "M8j_Np37OH_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head() # To have a look what your dataset looks like"
      ],
      "metadata": {
        "id": "NibUet_V_9ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To drop an unneccessary column\n",
        "dataset.drop(['your-column-name'], axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "LAVTHDyw8hbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To rename columns to have the standard text and label columns\n",
        "dataset = dataset.rename(columns={'tweets':'text'})"
      ],
      "metadata": {
        "id": "hFUBM0yQq7Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['label'].value_counts() # Returns each class counts"
      ],
      "metadata": {
        "id": "BBS6WkCjrXd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_sets = pd.concat([dataset1,dataset2]) # Returns datasets concatanated"
      ],
      "metadata": {
        "id": "USVXw6_MAaS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.dropna() # Returns the dataset dropping the NULL valued rows"
      ],
      "metadata": {
        "id": "FBlPq_SJBxz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.drop_duplicates(subset=['text'], keep='last') # Removes possible duplicate tweets and keeps the last occurence"
      ],
      "metadata": {
        "id": "lup-4Ja6DIvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.to_csv('your-dataset-name-to-be-saved.csv') # You can save your modified datasets to use later"
      ],
      "metadata": {
        "id": "3tIdGCOE-Xyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_dataset = shuffle(dataset) # You can shuffle your data, it is important to shuffle the data to mitigate order bias"
      ],
      "metadata": {
        "id": "f3aXamfX-qo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['text'] = dataset[df['label'] == 0].text # You can use this format to select rows on condition (like selecting entries that are not hate)"
      ],
      "metadata": {
        "id": "_NCzZacm_CTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can plot class distribution with countplot function of sns library"
      ],
      "metadata": {
        "id": "UmmgT8e_ETn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x=\"label\", data=dataframe)"
      ],
      "metadata": {
        "id": "Gp369kQvDHVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If dataset has labels like 'hate' 'nothate' 'True' 'False' etc., it is better to map those labels into binary values."
      ],
      "metadata": {
        "id": "qDlcfOjY-DmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['label'] = dataset['label'].map(lambda x:1 if x=='hate' else 0)"
      ],
      "metadata": {
        "id": "4KIkqMwY98eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to explore the frequent words"
      ],
      "metadata": {
        "id": "T9tyUo47D9uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "cnt = Counter()\n",
        "for text in dataset[\"text\"].values:\n",
        "    for word in text.split():\n",
        "        cnt[word] += 1\n",
        "\n",
        "cnt.most_common(10)"
      ],
      "metadata": {
        "id": "Z9qMgbR_D9S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or create a wordcloud"
      ],
      "metadata": {
        "id": "SkYoah9dELm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wordcloud\n",
        "from wordcloud import WordCloud\n",
        "words = ' '.join([txt for txt in dataset['text']])\n",
        "wordCloud = WordCloud(width=500, height=300, random_state=21, max_font_size=110).generate(words)\n",
        "\n",
        "plt.figure(figsize = (10, 8))\n",
        "plt.imshow(wordCloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4v3jefvOEK2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possible modifications in the text column"
      ],
      "metadata": {
        "id": "wV6LifMw_ivb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x['text'] = x['text'].str.lower() # Lowercasing is common preprocessing"
      ],
      "metadata": {
        "id": "_4znREyE_oSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "dataset[\"text\"] = dataset[\"text\"].apply(lambda text: remove_emoji(text))"
      ],
      "metadata": {
        "id": "xNj-tbotX21e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "dataset[\"text\"] = dataset[\"text\"].apply(lambda text: remove_urls(text))"
      ],
      "metadata": {
        "id": "aEYgM4Ora2k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "PUNCT_TO_REMOVE = string.punctuation\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
        "\n",
        "dataset[\"text\"] = dataset[\"text\"].apply(lambda text: remove_punctuation(text))"
      ],
      "metadata": {
        "id": "ASUHlWtdDeZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might also want to remove stop words if you are using another model architecture in training"
      ],
      "metadata": {
        "id": "QTWFSzZYDtAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "dataset[\"text\"] = dataset[\"text\"].apply(lambda text: remove_stopwords(text))"
      ],
      "metadata": {
        "id": "z4Qd-rwRDpRC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}