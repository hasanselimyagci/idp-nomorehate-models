{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKyPoavlCGcl"
      },
      "outputs": [],
      "source": [
        "pip install transformers datasets torch evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXCLTvNpEdvL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset,DatasetDict,load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n",
        "from transformers import DataCollatorWithPadding\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4A1fijXMSZ_"
      },
      "source": [
        "**LOADING THE DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VznuUBuKVWJ"
      },
      "outputs": [],
      "source": [
        "# dataset = load_dataset(\"selimyagci/dynamic-hate-speech-data\")\n",
        "# dataframe = pd.DataFrame(dataset['train'])\n",
        "dataframe = pd.read_csv('hateEn.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs2l9jojMaZD"
      },
      "source": [
        "**DATA OVERVIEW**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aksMF6aELp2F"
      },
      "outputs": [],
      "source": [
        "dataframe.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J8JmIo9NXh8"
      },
      "outputs": [],
      "source": [
        "dataframe['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiS8oHuwPeHc"
      },
      "outputs": [],
      "source": [
        "import wordcloud\n",
        "from wordcloud import WordCloud\n",
        "words = ' '.join([txt for txt in dataframe['text']])\n",
        "wordCloud = WordCloud(width=500, height=300, random_state=21, max_font_size=110).generate(words)\n",
        "\n",
        "plt.figure(figsize = (10, 8))\n",
        "plt.imshow(wordCloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhWoDbSXMsZL"
      },
      "source": [
        "**DATA PREPARATION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCzp192nRrGw"
      },
      "source": [
        "you can lower case all words for consistency\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4LAsfEfLxgl"
      },
      "outputs": [],
      "source": [
        "dataframe[\"text\"] = dataframe[\"text\"].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgB5fP95QOCx"
      },
      "outputs": [],
      "source": [
        "dataframe = dataframe.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjNMKqBn0BEz"
      },
      "outputs": [],
      "source": [
        "# You can remove non alphanumeric characters if desired\n",
        "pattern = '[^a-zA-Z0-9äöüÄÖÜß]'\n",
        "dataframe = dataframe.applymap(lambda x: re.sub(pattern, \" \", x) if pd.notnull(x) else x).sort_values('text')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEbCJeQgeFkt"
      },
      "source": [
        "Remove urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS2TGMQHeFK7"
      },
      "outputs": [],
      "source": [
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "dataframe[\"text\"] = dataframe[\"text\"].apply(lambda text: remove_urls(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGIPC7CM4MLp"
      },
      "source": [
        "*Removing emojis*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px_ogJbQ0F0j"
      },
      "outputs": [],
      "source": [
        "#dataframe = dataframe.drop(dataframe[dataframe['text'].str.isspace()].index)\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "dataframe[\"text\"] = dataframe[\"text\"].apply(lambda text: remove_emoji(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AST5jgbUSxtD"
      },
      "source": [
        "DATA SPLITTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVJo3HlmRSBb"
      },
      "outputs": [],
      "source": [
        "# You can change split sizes, following is 80-10-10 split\n",
        "train, tst = train_test_split(dataframe, test_size=0.2, shuffle=True)\n",
        "valid, test = train_test_split(tst, test_size=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzfJxSZLUBIX"
      },
      "source": [
        "**TOKENIZATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWXxwHilTSsD"
      },
      "outputs": [],
      "source": [
        "PRETRAINED = \"google-bert/bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0fHBJBoVkPQ"
      },
      "outputs": [],
      "source": [
        "def encode(docs):\n",
        "  encoded_dict = tokenizer.batch_encode_plus(docs,\n",
        "                                             add_special_tokens=True,\n",
        "                                             max_length=128,\n",
        "                                             padding='max_length',\n",
        "                                             return_attention_mask=True,\n",
        "                                             truncation=True,\n",
        "                                             return_tensors='pt')\n",
        "  input_ids = encoded_dict['input_ids']\n",
        "  attention_masks = encoded_dict['attention_mask']\n",
        "  return input_ids, attention_masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfVY2hyGV3lY"
      },
      "outputs": [],
      "source": [
        "train_input_ids, train_att_masks = encode(train['text'].values.tolist())\n",
        "valid_input_ids, valid_att_masks = encode(valid['text'].values.tolist())\n",
        "test_input_ids, test_att_masks = encode(test['text'].values.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JF9wTgyuV_6Z"
      },
      "outputs": [],
      "source": [
        "train_y = torch.LongTensor(train['label'].values.tolist())\n",
        "valid_y = torch.LongTensor(valid['label'].values.tolist())\n",
        "test_y = torch.LongTensor(test['label'].values.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFJNGcQwUv43"
      },
      "source": [
        "Creating dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5nemluzUvPe"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "BATCH_SIZE = 32 # You can change batch size i.e. 16, 64, 128, it affects the runtime and generalization\n",
        "train_dataset = TensorDataset(train_input_ids, train_att_masks, train_y)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "valid_dataset = TensorDataset(valid_input_ids, valid_att_masks, valid_y)\n",
        "valid_sampler = SequentialSampler(valid_dataset)\n",
        "valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_dataset = TensorDataset(test_input_ids, test_att_masks, test_y)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dr5NhxVoYIxu"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(PRETRAINED, num_labels=2,output_attentions=False,output_hidden_states=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKrVICBuY-32"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTHzwDwzZIWk"
      },
      "outputs": [],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bgi-bZRraJzk"
      },
      "source": [
        "**FINETUNING PRETRAINED MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzxK8nXiaG-M"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "EPOCHS = 3 # hyperparameter suggested between 2-5 epochs, if validation loss continues to decrease choose a higher epoch size\n",
        "LEARNING_RATE = 1e-5 # optimal hyperparameter value for learning rate, it is step size for optimizer\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "             num_warmup_steps=0,\n",
        "            num_training_steps=len(train_dataloader)*EPOCHS )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HejZ6CwDafxr"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils import clip_grad_norm_\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "train_loss_per_epoch = []\n",
        "val_loss_per_epoch = []\n",
        "\n",
        "\n",
        "for epoch_num in range(EPOCHS):\n",
        "    print('Epoch: ', epoch_num + 1)\n",
        "    '''\n",
        "    Training\n",
        "    '''\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for step_num, batch_data in enumerate(tqdm(train_dataloader,desc='Training')):\n",
        "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
        "        output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
        "\n",
        "        loss = output.loss\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        del loss\n",
        "\n",
        "        clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    train_loss_per_epoch.append(train_loss / (step_num + 1))\n",
        "\n",
        "\n",
        "    '''\n",
        "    Validation\n",
        "    '''\n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    valid_pred = []\n",
        "    with torch.no_grad():\n",
        "        for step_num_e, batch_data in enumerate(tqdm(valid_dataloader,desc='Validation')):\n",
        "            input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
        "            output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
        "\n",
        "            loss = output.loss\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            valid_pred.append(np.argmax(output.logits.cpu().detach().numpy(),axis=-1))\n",
        "\n",
        "    val_loss_per_epoch.append(valid_loss / (step_num_e + 1))\n",
        "    valid_pred = np.concatenate(valid_pred)\n",
        "\n",
        "    '''\n",
        "    Loss message\n",
        "    '''\n",
        "    print(\"{0}/{1} train loss: {2} \".format(step_num+1, math.ceil(len(train) / BATCH_SIZE), train_loss / (step_num + 1)))\n",
        "    print(\"{0}/{1} val loss: {2} \".format(step_num_e+1, math.ceil(len(valid) / BATCH_SIZE), valid_loss / (step_num_e + 1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training - Validation loss curves"
      ],
      "metadata": {
        "id": "FtFothQVM4eJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FGixUDocMq7"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "epochs = range(1, EPOCHS +1 )\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(epochs,train_loss_per_epoch,label ='training loss')\n",
        "ax.plot(epochs, val_loss_per_epoch, label = 'validation loss' )\n",
        "ax.set_title('Training and Validation loss')\n",
        "ax.set_xlabel('Epochs')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rEJJPEFdkZM"
      },
      "source": [
        "**Test the performance on test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-bF1_lPcmeZ"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "test_pred = []\n",
        "test_loss= 0\n",
        "with torch.no_grad():\n",
        "    for step_num, batch_data in tqdm(enumerate(test_dataloader)):\n",
        "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
        "        output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
        "\n",
        "        loss = output.loss\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        test_pred.append(np.argmax(output.logits.cpu().detach().numpy(),axis=-1))\n",
        "test_pred = np.concatenate(test_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywfuzW4AcqtT",
        "outputId": "067d20af-215b-48cc-b63f-9f588a893954"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      notmis       0.81      0.80      0.80       210\n",
            "         mis       0.78      0.79      0.78       190\n",
            "\n",
            "    accuracy                           0.79       400\n",
            "   macro avg       0.79      0.79      0.79       400\n",
            "weighted avg       0.79      0.79      0.79       400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(test_pred, test['label'].to_numpy(),target_names=['notmis','mis'])) # name your labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGy9o4TVdfJi"
      },
      "source": [
        "output misclassified ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mgvAXpdddmZ"
      },
      "outputs": [],
      "source": [
        "test['pred'] = test_pred\n",
        "test.reset_index(level=0)\n",
        "print(test[test['label']!=test['pred']].shape)\n",
        "test[test['label']!=test['pred']][['text','label','pred']].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKhoKjJ5d4y8"
      },
      "source": [
        "Saving the model\n",
        "- either pushing to Hugging Face hub\n",
        "- or you can save to your drive and download later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "179ubFhSi0RP"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub('user/your-model-name')\n",
        "tokenizer.push_to_hub('your-model-name')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99REelkbeVjN"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZdzRAMnd4Lp"
      },
      "outputs": [],
      "source": [
        "model_save_name = 'your-model-name.pt'\n",
        "path = F\"/content/gdrive/MyDrive/hate/{model_save_name}\"\n",
        "torch.save(model,path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}